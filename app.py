# app.py — The_305_Accountant (final, suggestions-only AI; transaction_id is canonical)
import io, json
import os, csv, traceback
from datetime import datetime
from flask import Flask, request, jsonify, Response, send_from_directory
from dotenv import load_dotenv
import pandas as pd
from database import autofill_subcategory_for_tx
from datetime import date
from database import initialize_database, apply_v1_compat_migrations, normalize_all_transaction_dates
from database import rebuild_fingerprints_and_dedupe, ensure_unique_fp_index
from werkzeug.exceptions import HTTPException
import sqlite3
from flask import current_app
from werkzeug.exceptions import NotFound
from datetime import datetime
from flask import Flask
import os

BASE_DIR     = os.path.dirname(os.path.abspath(__file__))
TEMPLATE_DIR = os.path.join(BASE_DIR, "templates")
STATIC_DIR   = os.path.join(BASE_DIR, "static")

app = Flask(__name__, template_folder=TEMPLATE_DIR, static_folder=STATIC_DIR)

from routes_import import import_bp
app.register_blueprint(import_bp)
@app.get("/favicon.ico")
def favicon():
    try:
        return send_from_directory(STATIC_DIR, "favicon.ico", mimetype="image/x-icon")
    except NotFound:
        return ("", 204)

def _fmt_mmddyy(iso_date: str) -> str:
    try:
        return datetime.strptime(iso_date or "", "%Y-%m-%d").strftime("%m-%d-%y")
    except Exception:
        return iso_date or ""


# project modules
from parser import intelligent_parser
from database import (
    initialize_database, apply_v1_compat_migrations,
    get_db_connection, get_or_create_account,
    list_accounts, fetch_transactions, fetch_summary, fetch_category_summary,
    add_transactions_df, import_corrections_from_rows,
    get_user_profile, set_user_profile,
    list_budgets, upsert_budget, estimate_budgets_from_history,
    get_budget_status, recompute_tracking_for_month,
    normalize_amount_signs, apply_category_rules, apply_rules_to_ai_fields,
    update_transaction_category_by_txid, normalize_all_transaction_dates,
    )

try:
    stats = normalize_all_transaction_dates()
    print(f"Date normalization on boot: {stats}")
except Exception as e:
    print(f"Date normalization skipped: {e}")

# optional AI (suggestions only)
try:
    from ai_categorizer import categorize_transactions_with_ai, openai_client
except Exception:
    categorize_transactions_with_ai = lambda: {"status":"error","message":"AI module missing or API key not set."}
    openai_client = None

# init
load_dotenv()
DATABASE = "finance.db"

def _to_mmddyy(s: str) -> str:
    if not s:
        return s
    for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%m-%d-%y", "%m/%d/%y"):
        try:
            return datetime.strptime(s, fmt).strftime("%m-%d-%y")
        except Exception:
            pass
    return s  # fallback unchanged

@app.errorhandler(Exception)
def api_json_errors(e):
    # Only wrap API routes; let normal pages keep HTML
    if request.path.startswith("/api/"):
        code = 500
        if isinstance(e, HTTPException):
            code = e.code
        app.logger.exception("API error on %s", request.path)
        return jsonify({"ok": False, "error": str(e)}), code
    # non-API: re-raise so Flask renders the HTML page/debugger
    raise e

@app.post("/api/corrections/import")
def api_import_corrections():
    """
    Accept a CSV generated by 'Export All to CSV' and apply corrections.
    Always returns JSON; never HTML.
    """
    try:
        f = request.files.get("file")
        if not f:
            return jsonify({"ok": False, "error": "No file uploaded (form field 'file' missing)."}), 400

        # Read whole upload & decode once — avoids SpooledTemporaryFile/TextIOWrapper gotchas
        text = f.read().decode("utf-8-sig", errors="ignore")
        rdr = csv.DictReader(io.StringIO(text))

        headers = [h.strip().lower() for h in (rdr.fieldnames or [])]
        if not headers:
            return jsonify({"ok": False, "error": "CSV has no header row."}), 400

        # Accept either 'id' or 'transaction_id'
        id_col = "id" if "id" in headers else ("transaction_id" if "transaction_id" in headers else None)
        if not id_col:
            return jsonify({"ok": False, "error": "CSV must include an 'id' (or 'transaction_id') column."}), 400

        conn = get_db_connection()
        updated = skipped = merged = 0

        # Ensure a backup table exists (for deletes on merge)
        conn.execute("""
            CREATE TABLE IF NOT EXISTS deleted_transactions AS
            SELECT *, '' AS deleted_at FROM transactions WHERE 1=0
        """)

        for row in rdr:
            tid_raw = (row.get(id_col) or "").strip()
            if not tid_raw.isdigit():
                skipped += 1
                continue
            tid = int(tid_raw)

            updates, params = [], []
            cd  = row.get("cleaned_description")
            cat = row.get("category")

            if cd is not None and cd.strip() != "":
                updates.append("cleaned_description=?")
                params.append(cd.strip())
            if cat is not None and cat.strip() != "":
                updates.append("category=?")
                params.append(cat.strip())

            if not updates:
                skipped += 1
                continue

            params.append(tid)

            try:
                conn.execute(f"UPDATE transactions SET {', '.join(updates)} WHERE id=?", params)
                updated += 1

            except sqlite3.IntegrityError as ie:
                # Merge-on-conflict for unique_fingerprint
                if "unique_fingerprint" in str(ie).lower():
                    fp = conn.execute("SELECT unique_fingerprint FROM transactions WHERE id=?", (tid,)).fetchone()
                    if fp and fp[0]:
                        other = conn.execute(
                            "SELECT id FROM transactions WHERE unique_fingerprint=? AND id<>?",
                            (fp[0], tid)
                        ).fetchone()
                        if other:
                            winner, loser = (min(tid, other["id"]), max(tid, other["id"]))
                            conn.execute(
                                "INSERT INTO deleted_transactions SELECT *, datetime('now') FROM transactions WHERE id=?",
                                (loser,)
                            )
                            conn.execute("DELETE FROM transactions WHERE id=?", (loser,))
                            merged += 1
                            continue
                # Not the fingerprint error: bubble up
                raise

        conn.commit()
        conn.close()
        return jsonify({"ok": True, "updated": updated, "skipped": skipped, "merged": merged})

    except Exception as e:
        try:
            current_app.logger.exception("API error on /api/corrections/import")
        finally:
            pass
        return jsonify({"ok": False, "error": str(e)}), 500


# ------------------- pages -------------------
@app.route("/")
def home():
    return send_from_directory(TEMPLATE_DIR, "index.html")

# ------------------- schema helper -------------------
@app.route("/api/export-schema", methods=["GET"])
def api_export_schema():
    """
    Returns the exact CSV header the Export/Corrections workflow uses.
    """
    cols = [
        "transaction_id", "date", "account", "description", "amount",
        "category",          # AI proposed (ai_category)
        "new_category",      # your final
        "new_description",   # merchant (canonical)
        "Sub_category"       # your final subcategory
    ]
    return jsonify({
        "export_columns": cols,
        "notes": "category = AI suggestion (ai_category). new_* columns are your selections."
    })

# ------------------- accounts ----------------
@app.route("/api/accounts", methods=["GET","POST"])
def api_accounts():
    conn = get_db_connection()
    try:
        if request.method == "POST":
            data = request.get_json() or {}
            name = (data.get("name") or "").strip()
            if not name:
                return jsonify({"error":"Account name is required."}), 400
            acc_id = get_or_create_account(conn, name)
            conn.commit()
            return jsonify({"id": acc_id, "name": name}), 201
        rows = conn.execute("SELECT id, name FROM accounts ORDER BY name").fetchall()
        return jsonify([dict(r) for r in rows])
    except Exception as e:
        conn.rollback()
        return jsonify({"error": f"Accounts operation failed: {e}"}), 500
    finally:
        conn.close()


# ------------------- transactions -------------
@app.route("/api/transactions", methods=["GET"])
def api_transactions():
    """
    Return transactions for the UI list.

    IMPORTANT:
    - Use LEFT JOIN so transactions still appear even if the accounts row is missing.
    - Keep global exclusion for 'Financial Transactions', but allow NULL category.
    """
    conn = get_db_connection()
    try:
        account_id = request.args.get("account_id")
        start_date = request.args.get("start_date")
        end_date   = request.args.get("end_date")

        params, where = [], []
        # exclude “Financial Transactions”; allow NULL
        where.append("COALESCE(t.category, '') != 'Financial Transactions'")

        if account_id:
            where.append("t.account_id = ?")
            params.append(account_id)
        if start_date:
            where.append("t.transaction_date >= ?")
            params.append(start_date)
        if end_date:
            where.append("t.transaction_date <= ?")
            params.append(end_date)

        where_sql = f" WHERE {' AND '.join(where)}" if where else ""
        order_sql = " ORDER BY t.transaction_date DESC, t.id DESC"

        sql = (
            "SELECT t.*, "
            "       COALESCE(a.name, 'Unknown') AS account_name, "
            "       CASE WHEN t.amount < 0 THEN 'debit' ELSE 'credit' END AS flow "
            "FROM transactions t "
            "LEFT JOIN accounts a ON a.id = t.account_id"
            f"{where_sql}{order_sql}"
        )
        rows = conn.execute(sql, tuple(params)).fetchall()
        out = []
        for r in rows:
            d = dict(r)
            d["transaction_date"] = _to_mmddyy(d.get("transaction_date", ""))
            out.append(d)
        return jsonify(out)
    except Exception as e:
        return jsonify({"error": f"Failed to fetch transactions: {e}"}), 500
    finally:
        conn.close()

# ------------------- inline update / quick rule upsert -------------
@app.route("/api/update-category", methods=["POST"])
def update_category():
    data = request.get_json() or {}
    # Accept either transaction_id or id, but prefer transaction_id (external key)
    txid = str(data.get("transaction_id") or data.get("id") or "").strip()
    new_category = (data.get("new_category") or "").strip()
    description = (data.get("description") or "").strip()

    if not txid or not new_category:
        return jsonify({"error": "Missing data for category update."}), 400

    conn = get_db_connection()
    try:
        # Update final category first
        conn.execute(
            "UPDATE transactions SET category = ? WHERE transaction_id = ?",
            (new_category, txid)
        )

        # Upsert a rule from your current description (merchant teaching)
        # We key by lower(description), same as the rest of the app
        if description:
            pattern = description.lower().strip()[:64]
            conn.execute(
                """
                INSERT INTO category_rules (merchant_pattern, category, subcategory, merchant_canonical)
                VALUES (?, ?, COALESCE(
                           (SELECT subcategory FROM category_rules WHERE merchant_pattern=? AND category=?),
                           NULL
                       ), ?)
                ON CONFLICT(merchant_pattern)
                DO UPDATE SET category=excluded.category,
                              merchant_canonical=COALESCE(excluded.merchant_canonical, category_rules.merchant_canonical)
                """,
                (pattern, new_category, pattern, new_category, description.strip())
            )

        conn.commit()
        conn.close()

        # Now auto-fill subcategory if a rule exists for (merchant, category)
        auto_sub = autofill_subcategory_for_tx(txid, new_category)

        return jsonify({
            "message": "Transaction and rule updated successfully.",
            "transaction_id": txid,
            "category": new_category,
            "auto_subcategory": auto_sub
        })
    except Exception as e:
        conn.rollback()
        conn.close()
        return jsonify({"error": f"Failed to update category: {str(e)}"}), 500


# ------------------- summary ------------------
@app.route("/api/summary", methods=["GET"])
def api_summary():
    """
    Balanced summary with explicit categories:
      - Income: category='Income'
      - Expenses: category NOT IN ('Income','Financial Transactions')  (negatives & any positives counted)
      - Savings: category='Savings'
      - Balance: SUM(all except 'Financial Transactions')
    """
    start_date = request.args.get("start_date", "").strip()
    end_date   = request.args.get("end_date", "").strip()
    conn = get_db_connection()
    try:
        args, where = [], []
        if start_date: where.append("transaction_date >= ?"); args.append(start_date)
        if end_date:   where.append("transaction_date <= ?"); args.append(end_date)
        clause = f"WHERE {' AND '.join(where)}" if where else ""

        income = conn.execute(
            f"SELECT COALESCE(SUM(amount),0) FROM transactions {clause} "
            f"{'AND' if clause else 'WHERE'} category = 'Income'", args
        ).fetchone()[0] or 0

        expenses = conn.execute(
            f"SELECT COALESCE(SUM(amount),0) FROM transactions {clause} "
            f"{'AND' if clause else 'WHERE'} category NOT IN ('Income','Financial Transactions')", args
        ).fetchone()[0] or 0

        savings = conn.execute(
            f"SELECT COALESCE(SUM(amount),0) FROM transactions {clause} "
            f"{'AND' if clause else 'WHERE'} category = 'Savings'", args
        ).fetchone()[0] or 0

        balance = conn.execute(
            f"SELECT COALESCE(SUM(amount),0) FROM transactions {clause} "
            f"{'AND' if clause else 'WHERE'} category != 'Financial Transactions'", args
        ).fetchone()[0] or 0

        return jsonify({"income": income, "expenses": expenses, "savings": savings, "balance": balance})
    except Exception as e:
        return jsonify({"error": f"Failed to compute summary: {e}"}), 500
    finally:
        conn.close()

# ------------------- category summary ----------
@app.route('/api/category-summary', methods=['GET'])
def api_category_summary():
    try:
        rows = fetch_category_summary(
            request.args.get('start_date'),
            request.args.get('end_date'),
            request.args.get('account_id')
        )
        return jsonify(rows)
    except Exception as e:
        return jsonify({"error": f"Failed to fetch category summary: {e}"}), 500

# ------------------- categories list ----------
@app.route("/api/categories", methods=["GET"])
def api_categories():
    conn = get_db_connection()
    try:
        rows = conn.execute(
            "SELECT DISTINCT category FROM transactions WHERE category IS NOT NULL AND category <> '' ORDER BY category"
        ).fetchall()
        cats = [r["category"] for r in rows] or [
            "Groceries","Dining","Transport","Bills & Utilities","General Merchandise",
            "Health & Wellness","Travel & Lodging","Entertainment","Home","Education",
            "Electronics","Card Payment","Income","Transfer","Other","Savings","Uncategorized"
        ]
        return jsonify(cats)
    except Exception as e:
        return jsonify({"error": str(e)}), 500
    finally:
        conn.close()

# ------------------- upload (bank CSVs) -------
# ------------------- upload (bank CSVs) -------
@app.route('/api/upload', methods=['POST'])
def upload_csv():
    """
    Smart uploader:
      - Accepts account_id OR 'csv' (read account from CSV column).
      - Parses CSV via intelligent_parser().
      - RAW-first: extract merchants (handles Zelle/Venmo/Cash App/PayPal/Apple Cash/Google Pay + Transfers) before insert.
      - Inserts rows (dedupes by transaction_id).
      - Normalizes signs & dates.
      - Applies learned rules (non-destructive).
      - Fills missing merchant via AI extractor (fallback).
      - Runs decisive AI categorizer (finals + learns rules).
      - Re-applies rules so subcategory/merchant standardize deterministically.
    """
    try:
        account_id = request.form.get('account_id') or request.args.get('account_id')
        if 'file' not in request.files or not account_id:
            return jsonify({"error": "Missing file or account_id."}), 400

        file = request.files['file']

        # ---- Parse the file ----
        df = intelligent_parser(io.BytesIO(file.read()))
        if df is None or df.empty:
            return jsonify({"error": "Could not parse the uploaded file."}), 400

        # --- FIXED INDENT (keep legacy block; do not clobber) -----------------
        # Ensure columns used later exist
        for c in ("merchant", "cleaned_description"):
            if c not in df.columns:
                df[c] = ""

        # 3a) Legacy Zelle prefill (kept): only fills where merchant is blank
        try:
            from parser import extract_zelle_to_from
            zmask = df["merchant"].astype(str).str.strip().eq("") & df["cleaned_description"].astype(str).str.contains("zelle", case=False, na=False)
            if zmask.any():
                df.loc[zmask, "merchant"] = df.loc[zmask, "cleaned_description"].map(extract_zelle_to_from).fillna("")
        except Exception as _e:
            # non-fatal
            pass

        # 3b) Legacy AI enrichment (kept): only fills where merchant is blank
        try:
            from ai_merchant_extractor import extract_merchant_names as _legacy_extract_merchants
            if os.getenv("ENRICH_MERCHANTS_ON_UPLOAD", "1") == "1":
                need_ai = df["merchant"].astype(str).str.strip().eq("") & df["cleaned_description"].astype(str).str.strip().ne("")
                if need_ai.any():
                    subset = df.loc[need_ai, "cleaned_description"].astype(str).tolist()
                    names = _legacy_extract_merchants(subset)  # returns aligned list
                    df.loc[need_ai, "merchant"] = names
        except Exception:
            # non-fatal
            pass
        # ----------------------------------------------------------------------

        # ---- RAW-FIRST merchant extraction (Transfers + P2P) BEFORE insert ----
        try:
            from ai_merchant_extractor import extract_merchants_from_dataframe
            import re

            # CHANGE: compute to temp series, then fill only blanks (no clobber)
            _extracted_list = extract_merchants_from_dataframe(
                df,
                use_columns=[
                    "original_description", "description", "cleaned_description",
                    "details", "narrative", "memo", "payee", "name", "transaction_description"
                ],
            )
            _extracted = pd.Series(_extracted_list, index=df.index)

            def _sanitize_unknown(x):
                if x is None:
                    return None
                s = str(x).strip().strip('"').strip("'")
                return None if (not s or s.lower() == "unknown") else s

            _extracted = _extracted.map(_sanitize_unknown)

            need_fill = df["merchant"].astype(str).str.strip().eq("") | df["merchant"].isna()
            if need_fill.any():
                df.loc[need_fill, "merchant"] = _extracted[need_fill]

            # --- mirror merchant into cleaned_description only when the current "cleaned" looks generic/raw ---
            GENERIC = {"zelle", "venmo", "cash app", "paypal", "apple cash", "google pay"}

            def _should_overwrite_cleaned(val: str | None) -> bool:
                if not val or not str(val).strip():
                    return True
                s = str(val).strip()
                low = s.lower()
                if low in GENERIC:
                    return True
                # obvious bank noise: ids/refs/long digits, etc.
                import re
                return bool(len(s) > 28 and re.search(r"(AUTHORIZED|REF|CARD|CONF|ID|TRACE|TXN|#|\d{5,}|[A-Z]{2,}\s*\d)", s, re.I))

            if "cleaned_description" not in df.columns:
                df["cleaned_description"] = df["merchant"]
            else:
                mask = df["cleaned_description"].astype(str).map(_should_overwrite_cleaned)
                df.loc[mask & df["merchant"].notna(), "cleaned_description"] = df.loc[mask, "merchant"]

        except Exception as e:
            # Non-fatal: DB heuristics / later steps will still run
            print(f"Pre-extract merchants skipped: {e}")

            if "merchant" in df.columns:
                df["merchant"] = df["merchant"].astype(str).str.strip()
                bad = df["merchant"].str.lower().isin(["unknown", "n/a", "na", "null", "none", "-"])
                df.loc[bad, "merchant"] = None

        # ---- Insert into DB (supports account_id='csv') ----
        added_total, skipped_total = 0, 0

        def _detect_account_col(frame):
            for c in ['account', 'Account', 'account_name', 'Account Name', 'acct']:
                if c in frame.columns:
                    return c
            return None

        if str(account_id).lower() == 'csv':
            acct_col = _detect_account_col(df)
            if not acct_col:
                return jsonify({"error": "account_id=csv requires an 'Account' column in the CSV."}), 400

            df[acct_col] = df[acct_col].astype(str).str.strip()
            for acct_name, subdf in df.groupby(acct_col):
                if not acct_name:
                    continue
                subdf = subdf.drop(columns=[acct_col])
                a, s = add_transactions_df(subdf, acct_name)
                added_total += a
                skipped_total += s
        else:
            # validate account id and get its name
            conn = get_db_connection()
            try:
                row = conn.execute('SELECT name FROM accounts WHERE id = ?', (account_id,)).fetchone()
            finally:
                conn.close()
            if not row:
                return jsonify({"error": "Account not found."}), 404

            a, s = add_transactions_df(df, row['name'])
            added_total += a
            skipped_total += s

        # ---- Hygiene passes: normalize signs & dates ----
        normalize_amount_signs()
        try:
            from database import normalize_all_transaction_dates as _norm_dates
            _ = _norm_dates()
        except Exception as e:
            print(f"Date normalization skipped: {e}")

        # ---- Apply existing rules to fill any blanks (non-destructive) ----
        try:
            from database import apply_category_rules
            apply_category_rules(overwrite=False)
        except Exception as e:
            print(f"Initial rule-apply skipped: {e}")

        # ---- Fill missing merchant names via AI extractor (fallback) ----
        filled_merchants = 0
        try:
            from ai_merchant_extractor import extract_merchant_names
            conn = get_db_connection()
            try:
                rows = conn.execute("""
                    SELECT transaction_id,
                           COALESCE(NULLIF(cleaned_description,''), original_description) AS text
                    FROM transactions
                    WHERE (merchant IS NULL OR TRIM(merchant) = '')
                      AND COALESCE(NULLIF(cleaned_description,''), original_description) IS NOT NULL
                    ORDER BY transaction_date DESC, id DESC
                    LIMIT 2000
                """).fetchall()
            finally:
                conn.close()

            if rows:
                texts = [r["text"] or "" for r in rows]
                ids   = [str(r["transaction_id"]) for r in rows]
                names = extract_merchant_names(texts)  # aligned list

                conn = get_db_connection()
                try:
                    for txid, name, src in zip(ids, names, texts):
                        # sanitize: never persist literal "Unknown"
                        safe_name = None
                        if isinstance(name, str):
                            s = name.strip().strip('"').strip("'")
                            if s and s.lower() != "unknown":
                                safe_name = s

                        conn.execute(
                            "UPDATE transactions "
                            "SET merchant = COALESCE(?, merchant), "
                            "    cleaned_description = COALESCE(NULLIF(cleaned_description,''), ?) "
                            "WHERE transaction_id = ?",
                            (safe_name, src, txid)
                        )

                    conn.commit()
                    filled_merchants = len(ids)
                finally:
                    conn.close()
        except Exception as e:
            print(f"Merchant extractor step skipped due to error: {e}")

        # ---- Run decisive AI categorizer (writes finals + learns rules) ----
        try:
            from ai_categorizer import categorize_transactions_with_ai
            ai_result = categorize_transactions_with_ai()
        except Exception as e:
            ai_result = {"status": "error", "message": f"AI step failed: {e}"}

        # ---- Re-apply rules so subcategory/merchant standardize deterministically ----
        try:
            from database import apply_category_rules
            apply_category_rules(overwrite=False)
        except Exception as e:
            print(f"Final rule-apply skipped: {e}")

        # ---- Response message ----
        msg = f"Imported {added_total} new, skipped {skipped_total} duplicates. "
        if filled_merchants:
            msg += f"Filled {filled_merchants} merchant names. "
        if isinstance(ai_result, dict) and ai_result.get("message"):
            msg += f"AI: {ai_result['message']}"

        return jsonify({"message": msg.strip()})

    except Exception as e:
        return jsonify({"error": f"Upload failed: {str(e)}"}), 500


    @app.route("/api/extract-merchants", methods=["POST"])
    def api_extract_merchants():
        """
        Fills transactions.merchant where it's empty, using the AI merchant extractor.
        - Selects rows where merchant is NULL/empty and cleaned_description is present.
        - Optional: ?limit=N (default 2000) to cap batch size.
        - Does NOT change categories; you can click 'Apply Learned Rules' next if you want subcategory fill/standardization.
        """
        try:
            limit = int(request.args.get("limit", 2000))
        except Exception:
            limit = 2000

        conn = get_db_connection()
        try:
            rows = conn.execute(
                """
                SELECT transaction_id, COALESCE(cleaned_description, original_description) AS src
                FROM transactions
                WHERE (merchant IS NULL OR TRIM(merchant) = '')
                AND TRIM(COALESCE(cleaned_description, original_description, '')) != ''
                ORDER BY transaction_date DESC, id DESC
                LIMIT ?
                """,
                (limit,),
            ).fetchall()

            if not rows:
                return jsonify({"message": "No blank-merchant rows found.", "updated": 0})

            texts = [r["src"] for r in rows]
            try:
                names = extract_merchant_names(texts)  # from ai_merchant_extractor.py
            except Exception as e:
                return jsonify({"error": f"Merchant extractor error: {e}"}), 500

            updated = 0
            for r, name in zip(rows, names):
                nm = (name or "").strip()
                if not nm:
                    continue
                conn.execute(
                    "UPDATE transactions SET merchant=? WHERE transaction_id=?",
                    (nm, str(r["transaction_id"]))
                )
                updated += 1

            conn.commit()
            return jsonify({"message": f"Updated merchant on {updated} transaction(s).", "updated": updated})
        finally:
            conn.close()

# ------------------- BOOTSTRAP: The Wholy Grail --------
@app.route("/api/bootstrap-grail", methods=["POST"])
def api_bootstrap_grail():
    """
    Load 'The Wholy Grail.csv' as authoritative history.
    - Uses per-row account names (creates accounts as needed).
    - Preserves final category + Sub_category from the CSV.
    - Requires: Transaction ID, Date, Amount columns (flexible names).
    """
    if "file" not in request.files:
        return jsonify({"error":"Missing file."}), 400
    raw = request.files["file"].read()
    if not raw:
        return jsonify({"error":"Empty file."}), 400

    try:
        import pandas as pd, io
        try:
            df = pd.read_csv(io.BytesIO(raw), dtype=str)
        except Exception:
            df = pd.read_csv(io.BytesIO(raw), dtype=str, encoding="latin-1")

        if df is None or df.empty:
            return jsonify({"error":"No rows in file."}), 400

        # ---------- column mapping (case-insensitive) ----------
        cols = {c.lower().strip(): c for c in df.columns}
        def pick(*names):
            for n in names:
                if n in cols: return cols[n]
            return None

        id_col   = pick("transaction_id","transaction id","id","txn id","txid","reference","reference number")
        date_col = pick("transaction_date","date","posting date","post date","posted date")
        amt_col  = pick("amount","transaction amount","amt")
        desc_col = pick("cleaned_description","description","original_description","memo","details","name","payee")
        # These are your FINAL labels in the Grail:
        final_cat_col = pick("new_category","category")  # prefer 'new_category' if present
        final_sub_col = pick("sub_category","Sub_category".lower(),"subcategory","sub category","sub cat","sub-cat","sub_cat","subcat","secondary category")
        acct_col = pick("account","account_name","account name","account#","acct","acct name","card name","bank account","account nickname")

        if not id_col or not date_col or not amt_col:
            return jsonify({"error":"File must include Transaction ID, Date, and Amount columns."}), 400

        # ---------- normalize ----------
        def S(s): return s.fillna("").astype(str).str.strip()
        df[id_col]   = S(df[id_col])
        df[date_col] = S(df[date_col])
        df[amt_col]  = S(df[amt_col])
        if desc_col:      df[desc_col]      = S(df[desc_col])
        if final_cat_col: df[final_cat_col] = S(df[final_cat_col])
        if final_sub_col: df[final_sub_col] = S(df[final_sub_col])
        if acct_col:      df[acct_col]      = S(df[acct_col])

        # ---------- normalized frame ----------
        import pandas as pd
        norm = pd.DataFrame({
            "transaction_id": df[id_col],
            "transaction_date": df[date_col],
            "amount": pd.to_numeric(df[amt_col], errors="coerce"),
            "cleaned_description": (df[desc_col] if desc_col else ""),
            "final_category": (df[final_cat_col] if final_cat_col else ""),
            "final_subcategory": (df[final_sub_col] if final_sub_col else ""),
            "account_name": (df[acct_col] if acct_col else "Grail")
        })
        norm = norm[(norm["transaction_id"]!="") & (norm["transaction_date"]!="") & norm["amount"].notna()]
        if norm.empty:
            return jsonify({"error":"After cleaning there were no valid rows to import."}), 400

        from database import add_transactions_df, normalize_amount_signs, apply_rules_to_ai_fields, get_db_connection, get_or_create_account

        # ---------- insert per account, then promote finals ----------
        added_total = skipped_total = 0
        per_account = []
        conn = get_db_connection()
        try:
            for acct, grp in norm.groupby(norm["account_name"].replace("", "Grail")):
                subdf = grp[["transaction_id","transaction_date","amount","cleaned_description"]].copy()
                # Insert rows (suggestions only; finals are set below)
                a, s = add_transactions_df(subdf, acct)
                added_total += a; skipped_total += s

                # Now write FINAL labels from the Grail into transactions
                # using the transaction_id as the key.
                updates = []
                for _, r in grp.iterrows():
                    cat = (r.get("final_category") or "").strip()
                    sub = (r.get("final_subcategory") or "").strip()
                    if cat or sub:
                        updates.append((cat if cat else None, sub if sub else None, str(r["transaction_id"]).strip()))
                if updates:
                    conn.executemany(
                        "UPDATE transactions SET "
                        "  category = COALESCE(?, category), "
                        "  subcategory = COALESCE(?, subcategory) "
                        "WHERE transaction_id = ?",
                        updates
                    )
                per_account.append({"account": acct, "added": int(a), "skipped": int(s)})

            # hygiene + suggestions
            normalize_amount_signs()
            apply_rules_to_ai_fields(conn)
            conn.commit()
        finally:
            conn.close()

        return jsonify({
            "message": "Grail bootstrap complete.",
            "added": int(added_total),
            "skipped": int(skipped_total),
            "accounts": per_account
        })
    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"Grail bootstrap failed: {e}"}), 500

# ------------------- AI (suggestions only) -----------------------
@app.route("/api/ai-categorize", methods=["POST"])
def api_ai_categorize():
    """
    Fills ai_category / ai_subcategory only (never touches final category/subcategory).
    """
    out = categorize_transactions_with_ai()
    return jsonify(out)

# Optional: AI financial report
@app.route("/api/ai-financial-report", methods=["POST"])
def ai_financial_report():
    if not openai_client:
        return jsonify({"error": "OpenAI client not initialized or AI module missing."}), 500
    conn = get_db_connection()
    rows = conn.execute(
        """
        SELECT transaction_id, transaction_date as date, cleaned_description, amount, category 
        FROM transactions
        WHERE category NOT IN ('Card Payment','Financial Transactions','Savings')
           OR category IS NULL
        ORDER BY transaction_date DESC
        """
    ).fetchall()
    conn.close()
    if not rows:
        return jsonify({"error": "No analyzable transactions available to generate a report."}), 400
    transaction_summary = [dict(r) for r in rows]
    prompt = (
        "You are a professional financial advisor. Analyze the following transactions and produce:\n"
        "1) Monthly Spending Analysis (top 3-5 categories & flags)\n"
        "2) Recommended Monthly Budget (with dollar amounts)\n"
        "3) 2-3 actionable recommendations.\n\n"
        f"Transactions JSON:\n{json.dumps(transaction_summary, indent=2)}"
    )
    try:
        resp = openai_client.chat.completions.create(
            model=os.getenv("OPENAI_REPORT_MODEL","gpt-4o"),
            messages=[{"role": "user", "content": prompt}],
        )
        return jsonify({"report": resp.choices[0].message.content})
    except Exception as e:
        return jsonify({"error": f"OpenAI API error: {str(e)}"}), 500

# ------------------- export -------------------
@app.route("/api/export", methods=["GET"])
def export_transactions():
    """
    Export all transactions (optionally filtered by start_date, end_date, account_id)
    to a CSV that includes the raw bank description for review/corrections.
    Dates are formatted MM-DD-YY for the export only.
    """
    # Local imports so we don't depend on a top-level `import database`
    from database import fetch_transactions
    import io, csv
    from datetime import datetime

    def fmt_mmddyy(iso_date: str) -> str:
        # Expecting YYYY-MM-DD in DB; fall back to whatever we got
        try:
            if not iso_date:
                return ""
            return datetime.strptime(str(iso_date), "%Y-%m-%d").strftime("%m-%d-%y")
        except Exception:
            return str(iso_date or "")

    # Optional filters
    start_date = request.args.get("start_date") or None
    end_date   = request.args.get("end_date") or None
    account_id = request.args.get("account_id") or None
    try:
        account_id = int(account_id) if account_id not in (None, "",) else None
    except Exception:
        account_id = None

    # Fetch as list[dict]
    rows = fetch_transactions(start_date=start_date, end_date=end_date, account_id=account_id)

    # Build CSV
    output = io.StringIO()
    writer = csv.writer(output)
    writer.writerow([
        "transaction_id",
        "date",                   # MM-DD-YY
        "account",
        "merchant",
        "original_description",   # RAW bank text
        "cleaned_description",
        "amount",
        "category",
        "new_category",           # empty for your corrections
        "new_description",        # empty for your corrections (merchant canonical)
        "Sub_category"            # empty for your corrections
    ])

    for r in rows:
        # rows are dicts (database.fetch_transactions returns dicts)
        writer.writerow([
            str(r.get("transaction_id", "")),
            fmt_mmddyy(r.get("transaction_date")),
            r.get("account_name", ""),
            r.get("merchant") or "",
            r.get("original_description") or "",
            r.get("cleaned_description") or "",
            f'{float(r.get("amount") or 0):.2f}',
            r.get("category") or "",
            "",   # new_category (to be filled in corrections sheet)
            "",   # new_description
            ""    # Sub_category
        ])

    output.seek(0)
    return Response(
        output.getvalue(),
        mimetype="text/csv",
        headers={"Content-Disposition": "attachment; filename=transactions.csv"}
    )
# ------------------- profile/budgets ----------
@app.route('/api/profile', methods=['GET','POST'])
def api_profile():
    try:
        if request.method == 'POST':
            data = request.get_json() or {}
            income = data.get('annual_after_tax_income')
            hh = int(data.get('household_size') or 1)
            cur = (data.get('currency') or 'USD').upper()
            set_user_profile(float(income) if income not in (None,"") else None, hh, cur)
        return jsonify(get_user_profile())
    except Exception as e:
        return jsonify({"error": f"Profile operation failed: {e}"}), 500

@app.route('/api/historical-spending', methods=['GET'])
def api_historical_spending():
    conn = get_db_connection()
    try:
        def avg_months(m):
            return conn.execute("""
                SELECT category, AVG(monthly_sum) AS avg_spend FROM (
                  SELECT strftime('%Y-%m', transaction_date) AS ym, category, SUM(amount) AS monthly_sum
                  FROM transactions
                  WHERE transaction_date >= date('now', ?)
                  GROUP BY ym, category
                )
                GROUP BY category
            """, (f'-{m} months',)).fetchall()
        data = {"1":{r["category"]:r["avg_spend"] or 0 for r in avg_months(1)},
                "3":{r["category"]:r["avg_spend"] or 0 for r in avg_months(3)},
                "6":{r["category"]:r["avg_spend"] or 0 for r in avg_months(6)},
                "18":{r["category"]:r["avg_spend"] or 0 for r in avg_months(18)}}
        return jsonify(data)
    except Exception as e:
        return jsonify({"error": f"Failed to fetch historical spending: {e}"}), 500
    finally:
        conn.close()

@app.route("/api/budgets", methods=["GET","POST"])
def api_budgets():
    try:
        if request.method == "POST":
            data = request.get_json() or {}
            for cat, limit in data.items():
                upsert_budget(cat, float(limit))
            recompute_tracking_for_month(datetime.today().strftime("%Y-%m"))
            return jsonify({"message":"Budget updated."})
        return jsonify(list_budgets())
    except Exception as e:
        return jsonify({"error": f"Budget operation failed: {e}"}), 500

@app.route('/api/propose-budget', methods=['POST'])
def api_propose_budget():
    try:
        months = int((request.get_json() or {}).get("months") or 3)
        return jsonify({"proposed_budget": estimate_budgets_from_history(months=months)})
    except Exception as e:
        return jsonify({"error": f"Failed to propose budget: {e}"}), 500

@app.route("/api/budget-status", methods=["GET"])
def api_budget_status():
    today = datetime.today()
    start_date = request.args.get("start_date", today.replace(day=1).strftime('%Y-%m-%d'))
    end_date   = request.args.get("end_date", today.strftime('%Y-%m-%d'))
    try:
        return jsonify(get_budget_status(start_date, end_date))
    except Exception as e:
        return jsonify({"error": f"Failed to compute budget status: {e}"}), 500

# ------------------- learn rules from labeled history (optional helper) ----
@app.route("/api/learn-rules-from-history", methods=["POST"])
def api_learn_rules_from_history():
    """
    Derive merchant rules from existing labeled transactions.
    Query params:
      ?min_count=N   (default 1)
      ?force=1       overwrite finals everywhere (careful!)
    Pattern = lower(merchant if present else cleaned_description) truncated to 64 chars.
    Rule stores: category, subcategory, merchant_canonical (most common non-empty merchant).
    """
    min_count = int(request.args.get("min_count", "1"))
    force = (request.args.get("force", "").lower() in ("1","true","yes"))

    conn = get_db_connection()
    try:
        # Pull labeled history
        rows = conn.execute("""
            SELECT
              lower(COALESCE(NULLIF(merchant,''), cleaned_description)) AS patt,
              category,
              COALESCE(subcategory,'') AS subcat,
              COALESCE(NULLIF(merchant,''), '') AS merch
            FROM transactions
            WHERE category IS NOT NULL AND category <> '' AND category <> 'Uncategorized'
        """).fetchall()

        from collections import defaultdict, Counter
        group = defaultdict(list)
        for r in rows:
            patt = (r["patt"] or "").strip()
            if not patt: continue
            group[patt[:64]].append((r["category"], r["subcat"], r["merch"]))

        upserts = 0
        for patt, items in group.items():
            # most common (category, subcat) and the most common non-empty merchant
            cat_sub_counts = Counter((c, s) for c, s, m in items)
            (best_cat, best_sub), best_cnt = cat_sub_counts.most_common(1)[0]
            if best_cnt < min_count:
                continue
            merch_counts = Counter(m for _, _, m in items if m)
            merchant_canonical = None
            if merch_counts:
                merchant_canonical = merch_counts.most_common(1)[0][0]
            conn.execute(
                "INSERT OR REPLACE INTO category_rules(merchant_pattern, category, subcategory, merchant_canonical) "
                "VALUES (?,?,?,?)",
                (patt, best_cat, (best_sub or None), merchant_canonical)
            )
            upserts += 1

        # apply rules (optionally standardize)
        apply_category_rules(conn, overwrite=force)
        conn.commit()
        return jsonify({"message":"Learned rules from history.",
                        "rules_upserted": upserts, "overwrite": force, "min_count": min_count})
    except Exception as e:
        conn.rollback()
        return jsonify({"error": f"Learning rules failed: {e}"}), 500
    finally:
        conn.close()

        # ---------- APPLY RULES (safe by default; overwrite with ?force=1) ----------
@app.route('/api/apply-rules', methods=['POST'])
def api_apply_rules():
    """
    Re-apply learned category rules to transactions.

    Default: only fills blanks / 'Uncategorized' / empty merchant.
    With ?force=1 : overwrite existing finals with rule values.
    """
    from database import get_db_connection, apply_category_rules  # lazy imports

    force = (request.args.get('force', '').lower() in ('1', 'true', 'yes'))

    conn = get_db_connection()
    try:
        before = conn.total_changes
        # Use the same connection so total_changes is accurate
        apply_category_rules(conn, overwrite=force)
        applied = conn.total_changes - before
        conn.commit()
        return jsonify({"applied": int(applied), "force": force})
    except Exception as e:
        conn.rollback()
        return jsonify({"error": f"Failed to apply rules: {e}"}), 500
    finally:
        conn.close()

@app.route("/api/debug/db-stats", methods=["GET"])
def api_debug_db_stats():
    """
    Quick inventory of what's in the DB so filters can be understood.
    Returns total count, min/max dates, and count for the current month.
    """
    conn = get_db_connection()
    try:
        start_month = date.today().replace(day=1).strftime("%Y-%m-%d")
        today = date.today().strftime("%Y-%m-%d")
        row = conn.execute(
            """
            SELECT
              COUNT(*)                            AS total_rows,
              MIN(transaction_date)               AS min_date,
              MAX(transaction_date)               AS max_date,
              SUM(CASE WHEN transaction_date BETWEEN ? AND ? THEN 1 ELSE 0 END)
                                                AS rows_this_month
            FROM transactions
            """,
            (start_month, today)
        ).fetchone()

        return jsonify({
            "total_rows": row["total_rows"] or 0,
            "min_date": row["min_date"],
            "max_date": row["max_date"],
            "rows_this_month": row["rows_this_month"] or 0,
        })
    finally:
        conn.close()

@app.route("/api/repair-accounts", methods=["POST"])
def api_repair_accounts():
    """
    Create placeholder accounts for any account_id referenced by transactions
    that doesn't exist in accounts. Names will be 'Restored #<id>'.
    """
    conn = get_db_connection()
    try:
        # insert missing accounts using explicit IDs
        cur = conn.execute("""
            INSERT INTO accounts(id, name)
            SELECT DISTINCT t.account_id, 'Restored #' || t.account_id
            FROM transactions t
            LEFT JOIN accounts a ON a.id = t.account_id
            WHERE a.id IS NULL
        """)
        conn.commit()
        return jsonify({"message": f"Repaired {cur.rowcount or 0} account(s)."})
    except Exception as e:
        conn.rollback()
        return jsonify({"error": f"Repair failed: {e}"}), 500
    finally:
        conn.close()

        

# ------------------- Zelle function ---------------------
@app.route('/api/fix-p2p-merchants', methods=['POST'])
def api_fix_p2p_merchants():
    """
    Normalize P2P merchants (Zelle/Venmo/Cash App/PayPal/Apple Cash/Google Pay)
    into the directional format, e.g. 'Zelle To John Doe' / 'Zelle From Jane Roe'.

    Query params:
      - force=1   : override even if merchant already set to some non-generic value
      - dry_run=1 : don't write, just report
      - limit=NN  : limit rows scanned (default 2000)
    """
    import re

    force   = (request.args.get('force', '').lower() in ('1', 'true', 'yes'))
    dry_run = (request.args.get('dry_run', '').lower() in ('1', 'true', 'yes'))
    try:
        limit = int(request.args.get('limit') or 2000)
    except Exception:
        limit = 2000

    # Use the SAME deterministic parser the extractor uses
    try:
        from ai_merchant_extractor import _p2p_merchant as p2p_prefill  # type: ignore
    except Exception:
        def p2p_prefill(text: str):
            s = text or ""
            provs = ["Zelle","Venmo","Cash App","PayPal","Apple Cash","Google Pay"]
            for prov in provs:
                if re.search(rf"\b{re.escape(prov)}\b", s, re.I):
                    mto = re.search(r"\bto\s+([A-Za-z@][A-Za-z0-9@.\-'\s]{1,80})", s, re.I)
                    mfr = re.search(r"\bfrom\s+([A-Za-z@][A-Za-z0-9@.\-'\s]{1,80})", s, re.I)
                    h   = re.search(r"@([A-Za-z0-9_.]{2,40})", s)
                    if mto: return f"{prov} To "   + mto.group(1).strip().title()
                    if mfr: return f"{prov} From " + mfr.group(1).strip().title()
                    if h:   return f"{prov} @{h.group(1)}"
                    return prov
            return None

    PROVIDERS_SET = {"zelle","venmo","cash app","paypal","apple cash","google pay"}

    def is_generic_p2p(val: str | None) -> bool:
        if not val: return True
        v = (val or "").strip().lower()
        return v in PROVIDERS_SET

    def richness(m: str | None) -> int:
        """Score a candidate: 0 = none, 1 = provider-only, 2 = has direction/handle."""
        if not m: return 0
        s = m.lower()
        if (" to " in s) or (" from " in s) or ("@" in s):
            return 2
        base = s.strip()
        return 1 if base in PROVIDERS_SET or base in [p.lower() for p in PROVIDERS_SET] else 2

    conn = get_db_connection()
    try:
        rows = conn.execute(
            """
            SELECT id, transaction_id,
                   original_description, cleaned_description, merchant
            FROM transactions
            WHERE
                 lower(COALESCE(cleaned_description,'')) LIKE '%zelle%'
              OR lower(COALESCE(original_description,''))  LIKE '%zelle%'
              OR lower(COALESCE(cleaned_description,'')) LIKE '%venmo%'
              OR lower(COALESCE(original_description,''))  LIKE '%venmo%'
              OR lower(COALESCE(cleaned_description,'')) LIKE '%cash app%'
              OR lower(COALESCE(original_description,''))  LIKE '%cash app%'
              OR lower(COALESCE(cleaned_description,'')) LIKE '%paypal%'
              OR lower(COALESCE(original_description,''))  LIKE '%paypal%'
              OR lower(COALESCE(cleaned_description,'')) LIKE '%apple cash%'
              OR lower(COALESCE(original_description,''))  LIKE '%apple cash%'
              OR lower(COALESCE(cleaned_description,'')) LIKE '%google pay%'
              OR lower(COALESCE(original_description,''))  LIKE '%google pay%'
            ORDER BY transaction_date DESC, id DESC
            LIMIT ?
            """,
            (limit,)
        ).fetchall()

        updated, skipped = 0, 0
        sample = []

        for r in rows:
            orig = (r["original_description"] or "").strip()
            clean = (r["cleaned_description"] or "").strip()
            current_m = (r["merchant"] or "").strip()

            # Try BOTH texts; pick the richer candidate
            cand_orig  = p2p_prefill(orig)
            cand_clean = p2p_prefill(clean)
            candidate  = cand_orig if richness(cand_orig) >= richness(cand_clean) else cand_clean

            if not candidate:
                skipped += 1
                continue

            # Decide if we should write
            if force:
                do_update = (current_m != candidate) or is_generic_p2p(clean)
            else:
                do_update = (not current_m) or is_generic_p2p(current_m)

            if not do_update:
                skipped += 1
                continue

            if dry_run:
                sample.append({
                    "transaction_id": r["transaction_id"],
                    "from": current_m or clean or orig,
                    "to": candidate
                })
            else:
                # Write merchant; update cleaned_description only if generic/empty
                conn.execute(
                    """
                    UPDATE transactions
                       SET merchant = ?,
                           cleaned_description = CASE
                               WHEN (? OR cleaned_description IS NULL OR TRIM(cleaned_description)='')
                                 THEN ?
                               ELSE cleaned_description
                           END
                     WHERE id = ?
                    """,
                    (candidate, is_generic_p2p(clean), candidate, r["id"])
                )
                updated += 1

        if not dry_run:
            conn.commit()

        return jsonify({
            "scanned": len(rows),
            "force": force,
            "dry_run": dry_run,
            "updated": updated,
            "skipped": skipped,
            "sample": sample[:25],
        })

    except Exception as e:
        try: conn.rollback()
        except Exception: pass
        return jsonify({"error": f"Failed to fix P2P merchants: {e}"}), 500
    finally:
        conn.close()

@app.route('/api/debug/extract-merchants', methods=['POST'])
def api_debug_extract_merchants():
    """
    Debug the merchant extractor on arbitrary strings you provide.
    Body (JSON):
      {
        "texts": ["Zelle Payment To john doe memo 123", "Venmo from @maria thanks", ...],
        "use_ai": true|false   # optional; default false (deterministic only)
      }
    """
    body = request.get_json(silent=True) or {}
    texts = body.get('texts') or []
    use_ai = bool(body.get('use_ai', False))

    if not isinstance(texts, list) or not texts:
        return jsonify({"error": "Provide JSON body with a non-empty 'texts' array."}), 400

    try:
        from ai_merchant_extractor import debug_parse_p2p, extract_merchant_names
    except Exception as e:
        return jsonify({"error": f"Extractor not available: {e}"}), 500

    # deterministic view
    dbg = debug_parse_p2p([str(t or "") for t in texts])

    # optional AI pass (to compare with deterministic)
    ai_list = None
    if use_ai:
        try:
            ai_list = extract_merchant_names([str(t or "") for t in texts])
        except Exception as e:
            ai_list = None

    # combine
    items = []
    for i, _ in enumerate(texts):
        row = dict(dbg[i])
        row["ai_merchant"] = (ai_list[i] if (ai_list and i < len(ai_list)) else None)
        # final_decision mirrors extractor logic: prefer deterministic prefill when present
        row["final_decision"] = row["prefill_merchant"] or row["ai_merchant"]
        items.append(row)

    return jsonify({"count": len(items), "items": items})

@app.route('/api/debug/p2p-scan', methods=['GET'])
def api_debug_p2p_scan():
    """
    Peek at DB rows that look like P2P by raw description and show how the
    deterministic P2P parser would label them (no DB writes).
    Query params:
      - limit (default 50)
    """
    limit = int(request.args.get('limit', 50))

    conn = get_db_connection()
    try:
        rows = conn.execute("""
            SELECT transaction_id,
                   transaction_date,
                   original_description,
                   cleaned_description,
                   merchant
            FROM transactions
            WHERE lower(COALESCE(original_description,'')) GLOB '*zelle*'
               OR lower(COALESCE(original_description,'')) GLOB '*venmo*'
               OR lower(COALESCE(original_description,'')) GLOB '*cash app*'
               OR lower(COALESCE(original_description,'')) GLOB '*cashapp*'
               OR lower(COALESCE(original_description,'')) GLOB '*paypal*'
               OR lower(COALESCE(original_description,'')) GLOB '*pypl*'
               OR lower(COALESCE(original_description,'')) GLOB '*apple cash*'
               OR lower(COALESCE(original_description,'')) GLOB '*apple pay*'
               OR lower(COALESCE(original_description,'')) GLOB '*gpay*'
               OR lower(COALESCE(original_description,'')) GLOB '*google pay*'
            ORDER BY transaction_date DESC, id DESC
            LIMIT ?
        """, (limit,)).fetchall()
    finally:
        conn.close()

    texts = []
    items = []
    for r in rows:
        # Prefer original_description for P2P detection; fall back to cleaned
        source = (r["original_description"] or r["cleaned_description"] or "")
        texts.append(source)
        items.append({
            "transaction_id": r["transaction_id"],
            "transaction_date": r["transaction_date"],
            "merchant_current": r["merchant"],
            "source_text": source,
        })

    try:
        from ai_merchant_extractor import debug_parse_p2p
        dbg = debug_parse_p2p(texts)
    except Exception as e:
        return jsonify({"error": f"Extractor not available: {e}"}), 500

    # stitch debug back onto the rows
    for i, d in enumerate(dbg):
        items[i]["provider"] = d["provider"]
        items[i]["direction"] = d["direction"]
        items[i]["counterparty"] = d["counterparty"]
        items[i]["prefill_merchant"] = d["prefill_merchant"]

    return jsonify({"count": len(items), "items": items})


@app.route("/api/admin/rebuild-fingerprints", methods=["POST"])
def api_admin_rebuild_fingerprints():
    """
    Recompute global fingerprints and (optionally) delete duplicates.
    POST ?dry_run=1 to preview.
    """
    dry = (request.args.get("dry_run", "0").lower() in ("1","true","yes"))
    from database import rebuild_fingerprints_and_dedupe, _drop_unique_fingerprint_index, _create_unique_fingerprint_index
    try:
        out = rebuild_fingerprints_and_dedupe(dry_run=dry)
        return jsonify(out)
    except Exception as e:
        # If a UNIQUE error slipped through, try once more after forcing a drop
        if "UNIQUE constraint failed: transactions.unique_fingerprint" in str(e):
            conn = get_db_connection()
            try:
                _drop_unique_fingerprint_index(conn)
                conn.commit()
            finally:
                conn.close()
            out = rebuild_fingerprints_and_dedupe(dry_run=dry)
            return jsonify(out)
        raise

@app.route("/api/admin/enforce-unique-fingerprint", methods=["POST"])
def api_admin_enforce_unique_fingerprint():
    from database import get_db_connection, _create_unique_fingerprint_index
    conn = get_db_connection()
    try:
        _create_unique_fingerprint_index(conn)
        conn.commit()
        return jsonify({"created_or_exists": True})
    finally:
        conn.close()


# ------------------- main ---------------------
if __name__ == "__main__":
    if not os.path.exists(DATABASE):
        initialize_database()
    else:
        initialize_database()  # safe no-op if exists
    apply_v1_compat_migrations()  # <-- ensure columns on every boot
    app.run(host="0.0.0.0", port=5056, debug=True, use_reloader=False)
# === Staged import endpoints ===
from flask import request, jsonify, send_file, render_template
import json, io, csv, sqlite3, hashlib, os
import database
from plaid_integration import create_link_token, exchange_public_token, transactions_get_by_date
from database import get_or_create_account

def _rule_suggest(cat_rules, desc: str):
    d = (desc or "").lower().strip()
    for pat, cat in cat_rules:
        if pat and pat.lower() in d:
            return {"merchant": None, "category": cat, "sub_category": ""}
    return None

def _ai_suggest(desc, amount):
    if not os.getenv("ENABLE_AI") or os.getenv("ENABLE_AI") == "0":
        return None
    try:
        from openai import OpenAI
        client = OpenAI()
        # patched to avoid f-string braces parsing
        prompt = (
            "Merchant+category suggestion.\n"
            f"Description: {desc}\n"
            f"Amount: {amount}\n"
            "Return JSON like {\"merchant\":\"\",\"category\":\"\",\"sub_category\":\"\"}."
        )
        msg = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role":"user","content":prompt}],
            temperature=0.1
        )
        data = json.loads(msg.choices[0].message.content.strip())
        return {"merchant": data.get("merchant",""), "category": data.get("category",""), "sub_category": data.get("sub_category","")}
    except Exception:
        return None

@app.post("/import/plaid/start")
def import_plaid_start():
    p = request.get_json(force=True)
    item_id = p["item_id"]; start = p["start_date"]; end = p["end_date"]; account_ids = p.get("account_ids")
    got = transactions_get_by_date(item_id, start, end, account_ids)
    if "error" in got: return jsonify(got), 400
    txns = got["transactions"]
    con = database.get_db_connection(); cur = con.cursor()
    cur.execute("""INSERT INTO import_batches (source,item_id,account_ids,start_date,end_date,status)
                   VALUES ('plaid',?,?,?,?, 'raw')""", (item_id, json.dumps(account_ids or []), start, end))
    batch_id = cur.lastrowid
    ins = 0
    for t in txns:
        try:
            cur.execute("""
            INSERT OR IGNORE INTO import_raw
            (batch_id, plaid_txn_id, account_id, date, authorized_date, name, merchant_name, amount, pending, currency, original_json)
            VALUES (?,?,?,?,?,?,?,?,?,?,?)
            """, (batch_id, t.get("transaction_id"), t.get("account_id"),
                  t.get("date"), t.get("authorized_date"),
                  t.get("name"), t.get("merchant_name"),
                  float(t.get("amount") or 0.0), 1 if t.get("pending") else 0,
                  (t.get("iso_currency_code") or t.get("unofficial_currency_code") or "USD"),
                  json.dumps(t)))
            ins += cur.rowcount
        except Exception:
            pass
    con.commit(); con.close()
    return jsonify({"batch_id": batch_id, "raw_inserted": ins})

@app.post("/import/suggest/<int:batch_id>")
def import_suggest(batch_id):
    con = database.get_db_connection(); cur = con.cursor()
    rows = cur.execute("SELECT id,name,merchant_name,amount FROM import_raw WHERE batch_id=?", (batch_id,)).fetchall()
    rules = cur.execute("SELECT merchant_pattern, category FROM category_rules").fetchall()
    made = 0
    for rid, name, merch, amt in rows:
        s = _rule_suggest(rules, name or merch or "")
        src = None
        if s:
            src = "rule"
        else:
            s_ai = _ai_suggest(name or merch or "", amt)
            if s_ai:
                s = {"merchant": s_ai["merchant"], "category": s_ai["category"], "sub_category": s_ai.get("sub_category","")}
                src = "ai"
        if s:
            cur.execute("""
            INSERT OR REPLACE INTO import_suggestions
            (batch_id, raw_id, suggested_merchant, suggested_category, suggested_subcategory, confidence, source)
            VALUES (?,?,?,?,?,?,?)
            """, (batch_id, rid, s.get("merchant") or "", s.get("category") or "", s.get("sub_category") or "", 0.9 if src=="rule" else 0.6, src))
            made += 1
    cur.execute("UPDATE import_batches SET status='suggested' WHERE id=?", (batch_id,))
    con.commit(); con.close()
    return jsonify({"batch_id": batch_id, "suggested": made})

@app.get("/import/review/<int:batch_id>.json")
def import_review_json(batch_id):
    con = database.get_db_connection(); cur = con.cursor()
    q = """
    SELECT r.id as raw_id, r.date, r.name, r.merchant_name, r.amount, r.pending,
           COALESCE(s.suggested_merchant,'') AS merchant,
           COALESCE(s.suggested_category,'') AS category,
           COALESCE(s.suggested_subcategory,'') AS sub_category,
           COALESCE(s.source,'') AS source
    FROM import_raw r
    LEFT JOIN import_suggestions s ON s.raw_id=r.id
    WHERE r.batch_id=?
    ORDER BY r.date, r.amount DESC
    """
    rows = [dict(zip([c[0] for c in cur.description], x)) for x in cur.execute(q, (batch_id,)).fetchall()]
    con.close()
    return jsonify({"batch_id": batch_id, "rows": rows})

@app.get("/import/review/<int:batch_id>.csv")
def import_review_csv(batch_id):
    con = database.get_db_connection(); cur = con.cursor()
    q = """
    SELECT r.date, r.name, r.merchant_name, r.amount, r.pending,
           COALESCE(s.suggested_merchant,'') AS merchant,
           COALESCE(s.suggested_category,'') AS category,
           COALESCE(s.suggested_subcategory,'') AS sub_category
    FROM import_raw r LEFT JOIN import_suggestions s ON s.raw_id=r.id
    WHERE r.batch_id=? ORDER BY r.date, r.amount DESC
    """
    rows = cur.execute(q, (batch_id,)).fetchall()
    con.close()
    out = io.StringIO(); w = csv.writer(out)
    w.writerow(["date","name","merchant_name","amount","pending","merchant","category","sub_category"])
    for r in rows: w.writerow(r)
    mem = io.BytesIO(out.getvalue().encode("utf-8")); mem.seek(0)
    return send_file(mem, mimetype="text/csv", as_attachment=True, download_name=f"batch_{batch_id}_review.csv")

@app.post("/import/commit/<int:batch_id>")
def import_commit(batch_id):
    p = request.get_json(force=True) if request.data else {}
    overrides = {int(o["raw_id"]): o for o in p.get("overrides", [])}
    dup_policy = (p.get("duplicate_policy") or "skip").lower()
    con = database.get_db_connection(); cur = con.cursor()
    q = """
    SELECT r.id, r.date, r.name, r.merchant_name, r.amount, r.account_id, r.plaid_txn_id,
           COALESCE(s.suggested_merchant,''), COALESCE(s.suggested_category,''), COALESCE(s.suggested_subcategory,'')
    FROM import_raw r LEFT JOIN import_suggestions s ON s.raw_id=r.id
    WHERE r.batch_id=?
    """
    rows = cur.execute(q, (batch_id,)).fetchall()
    # discover columns present in transactions (so we can insert safely)
    cols = {r[1].lower(): r[1] for r in cur.execute("PRAGMA table_info('transactions')")}
    has = lambda c: c in cols
    inserted = 0; skipped = 0
    for (raw_id, dt, nm, merch, amt, plaid_acct, plaid_txn_id, sug_merch, sug_cat, sug_sub) in rows:
        ov = overrides.get(int(raw_id))
        merchant = (ov and ov.get("merchant")) or (sug_merch or merch or nm or "")
        category = (ov and ov.get("category")) or (sug_cat or "")
        subcat   = (ov and ov.get("sub_category")) or (sug_sub or "")
        local_acct_id = get_or_create_account(con, plaid_acct or "Plaid Account")
        # duplicate guard
        if dup_policy == "skip":
            if has("plaid_txn_id"):
                exists = cur.execute("SELECT 1 FROM transactions WHERE plaid_txn_id=? LIMIT 1", (plaid_txn_id,)).fetchone() if plaid_txn_id else None
                if exists: skipped += 1; continue
            exists = cur.execute("""
              SELECT 1 FROM transactions
              WHERE transaction_date=? AND amount=? AND original_description=? AND account_id=?
              LIMIT 1
            """, (dt, amt, nm, local_acct_id)).fetchone()
            if exists: skipped += 1; continue
        # build insert dynamically
        fields = ["transaction_date","original_description","cleaned_description","amount","category","sub_category","account_id","unique_hash"]
        vals   = [dt, nm, merchant, float(amt), category, subcat, local_acct_id, hashlib.sha256(f"{local_acct_id}|{dt}|{(nm or '').lower()}|{float(amt):.2f}|{plaid_txn_id or ''}".encode()).hexdigest()]
        if has("transaction_id"):
            fields.append("transaction_id"); vals.append(plaid_txn_id)
        if has("plaid_txn_id"):
            fields.append("plaid_txn_id");   vals.append(plaid_txn_id)
        sql = f"INSERT INTO transactions ({','.join(fields)}) VALUES ({','.join(['?']*len(vals))})"
        cur.execute(sql, vals); inserted += 1
    cur.execute("UPDATE import_batches SET status='committed' WHERE id=?", (batch_id,))
    con.commit(); con.close()
    return jsonify({"batch_id": batch_id, "inserted": inserted, "skipped": skipped})

@app.post("/import/discard/<int:batch_id>")
def import_discard(batch_id):
    con = database.get_db_connection(); cur = con.cursor()
    cur.execute("DELETE FROM import_suggestions WHERE batch_id=?", (batch_id,))
    cur.execute("DELETE FROM import_raw WHERE batch_id=?", (batch_id,))
    cur.execute("UPDATE import_batches SET status='discarded' WHERE id=?", (batch_id,))
    con.commit(); con.close()
    return jsonify({"ok": True})

# simple page to drive the flow
@app.get("/plaid_import")
def plaid_import_page():
    return render_template("plaid_import.html")


from flask import request, jsonify, render_template
from plaid_integration import create_link_token, exchange_public_token, transactions_get_by_date
def create_link():
    user_id = "user-1"
    return jsonify({"link_token": create_link_token(user_id)})

@app.post("/plaid/exchange_public_token")
def exchange_public():
    public_token = (request.json or {}).get("public_token")
    if not public_token:
        return jsonify({"error":"missing public_token"}), 400
    return jsonify(exchange_public_token(public_token))

@app.post("/plaid/transactions/sync/<item_id>")
def plaid_sync(item_id):
    from flask import jsonify
    return jsonify({"error":"Plaid sync not enabled"}), 501

@app.get("/plaid/oauth/callback")
def plaid_oauth_callback():
    return render_template("dashboard.html")



@app.get("/plaid")
def plaid_page():
    from flask import render_template
    return render_template("plaid.html")
